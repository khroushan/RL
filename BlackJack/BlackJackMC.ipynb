{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to generate Fig 5.1 in Reinforcement Learning by Andrew Barto and Richard S. Sutton\n",
    "# Ch.5 Monte-Carlo prediction, algorithm: First-visit MC prediction\n",
    "# gym api is used\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "# importing Black-Jack game API\n",
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An observation in `Blackjack` consists of \n",
    " 1. player's sum \n",
    " 2. dealer's face-up card\n",
    " 3. if player is natural or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_dict(d):\n",
    "    \"\"\" Return the maximum value and its key \"\"\"\n",
    "    max_v = float('-inf')\n",
    "    for key, val in d.items():\n",
    "        if val > max_v:\n",
    "            max_v = val\n",
    "            max_key = key\n",
    "    return max_key, max_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_as_string(state):\n",
    "    \"\"\" Convert the given observation into string of form\n",
    "    [][][][][][]\n",
    "    \"\"\"\n",
    "    string_state = ''.join(str(int(e)).zfill(2) for e in state)\n",
    "    return string_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_states_as_string():\n",
    "    \"\"\" Function to generate all possible states and assign a \n",
    "    string to each states \n",
    "    ex: (20, 8, False) --> 200800\n",
    "    \"\"\"\n",
    "    states = []\n",
    "    for i in range(1, 33):\n",
    "        for j in range(1,12):\n",
    "            for k in [False, True]:\n",
    "                states.append(str(int(i)).zfill(2) + str(int(j)).zfill(2) + \n",
    "                              str(int(k)).zfill(2))\n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize state value\n",
    "def initialize_Q():\n",
    "    \"\"\" Initialize the state-action value dictionary.\n",
    "    \"\"\"\n",
    "    Q = {}\n",
    "    \n",
    "    all_states = get_all_states_as_string()\n",
    "    for state in all_states:\n",
    "        Q[state] = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            Q[state][action] = 0\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(observation, Q):\n",
    "    \"\"\" Generate an action based on previous\n",
    "    state. Stick if sum is 20 or 21\n",
    "    \"\"\"\n",
    "    state = get_state_as_string(observation)\n",
    "    action = max_dict(Q[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing first visit Monte-Carlo for estimating state value $v_{\\pi}(s)$ according to  policy $\\pi$.\n",
    "Since in BlackJack game, every state is visited only once, there is no need to check if it is a first visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte-Carlo simulation \n",
    "def MC_stateValue():    \n",
    "    Q = initialize_Q()\n",
    "    for _ in range(1000000):\n",
    "        # policy: stick just when the player sum is 20 or 21\n",
    "        done = False\n",
    "        # Exploring start:\n",
    "        observation = env.reset()\n",
    "        # generating new episode\n",
    "        while not done:\n",
    "            action, _ = policy(observation, Q)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            state = get_state_as_string(observation)\n",
    "            Q[state][action] += reward\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = MC_stateValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive the optimal policy from Q\n",
    "# not natural\n",
    "opt_policy = np.zeros((12, 12), dtype=int)\n",
    "natural = False\n",
    "for i in range(11, 22):  # player's sum\n",
    "    for j in range(1, 12):  # dealer's card\n",
    "        state = str(int(i)).zfill(2) + str(int(j)).zfill(2) + \\\n",
    "                str(int(natural)).zfill(2)\n",
    "        opt_policy[i-11,j-1] = max_dict(Q[state])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# derive the optimal policy from Q\n",
    "# natural\n",
    "opt_policy = np.zeros((12, 12), dtype=int)\n",
    "natural = True\n",
    "for i in range(11, 22):  # player's sum\n",
    "    for j in range(1, 12):  # dealer's card\n",
    "        state = str(int(i)).zfill(2) + str(int(j)).zfill(2) + \\\n",
    "                str(int(natural)).zfill(2)\n",
    "        opt_policy[i-11,j-1] = max_dict(Q[state])[0]\n",
    "opt_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = pl.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "y = np.linspace(11,21,11)\n",
    "x = np.linspace(1,11, 11)\n",
    "X, Y = np.meshgrid(x,y)\n",
    "ax.plot_wireframe(X, Y, Qarr, antialiased=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
